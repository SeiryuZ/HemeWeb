% Activate the following line by filling in the right side. If for example the name of the root file is Main.tex, write
% "...root = Main.tex" if the chapter file is in the same directory, and "...root = ../Main.tex" if the chapter is in a subdirectory.
 
%!TEX root =  dissertation.tex

\chapter[Background]{Background}

In this chapter, I will discuss about various background information that are used as a basis for the work presented in this dissertation. I will discuss about how HemeLB currently works, High-Performance Computing (HPC) infrastructure, and docker.


\section{Current HemeLB workflow}

Currently, running a blood flow simulation consists of multiple steps that needs to be run in sequence. These steps are done in a variety of interface, from command line to graphical user interface. Additionally, these steps also require various level of computing resources to work efficiently. In order to understand how the proposed work will improve the current conditions, I will elaborate on how HemeLB currently work. Also, discussion on computing resources and interface for each steps will be provided.

%Running a blood flow simulation using HemeLB currently consists of multiple steps. To understand how the proposed project can improve the current conditions, I will elaborate on how HemeLB workflow currently work based on the work on my proposal\cite{Steven:2016aa}.



\vspace{1cm}

\noindent%
\begin{minipage}{\linewidth}% to keep image and caption on one page
\makebox[\linewidth]{
  \includegraphics[keepaspectratio=true,scale=0.6]{../resources/images/HemeLB-workflow.png}
 }
\captionof{figure}{Current HemeLB workflow taken from \cite{Steven:2016aa}}\label{fig:hemelb-workflow}%      only if needed  
\end{minipage}

\vspace{1cm}


Figure \ref{fig:hemelb-workflow} illustrates steps involved in running HemeLB workflow. These steps will be discussed in details below:

\begin{enumerate}

\item{\textbf{Geometrical model reconstruction}}

In this step, a 3D model of vascular system is constructed from the raw microscopic image of it. Alternatively, the 3D model can also be constructed from CT scan with its 3D imaging data. From this step, a 3D geometry file are generated in the form of .stl file. This process can run in a regular workstation just fine. However, it is highly problem dependent as the tools needed to parse and generate the 3D model are dependent to the problem experts try to simulate.

\item{\textbf{Domain definition}}

3D geometry model generated from the previous step is now used as an input for the domain definition step. In this step, a graphical user interface is used to add domain information to the 3D model. Information like blood viscosity, inlet outlet placement, and blood pressure will determine how the simulation will run. The HemeLB setup tool was developed for this particular needs. The setup tool provides a graphical user interface for domain experts to add these parameters. All parameters are then saved in a profile file with .pr2 format. This step can run on standalone commodity hardware and should not require a highly parallel computing resources.

\item{\textbf{Geometry generation}}

This step will take the encoded information from domain definition step and the 3D model of the vascular system to generate files that can be understood by the main HemeLB program. These files contain similar information with the previous 3D model and the profile file. However, both of them are now formatted in a HemeLB parseable format, an XML configuration file and a GMY geometry file. This geometry generation step can also run on a commodity hardware. However, it requires users to use command line interface to operate with the files. The process is done with piping the input files to a python script which is part of HemeLB setup tools.

\item{\textbf{HemeLB simulation}}

The main heavy computations of the workflow are done in this step.  Configuration and geometry files that are generated in the previous step are feed into the HemeLB binary as input files. HemeLB will then run calculations that govern how blood will flow inside the provided vascular system for a number of iterative steps. The number of steps is defined in the configuration file that is generated in the domain definition steps. 

As observed in the proposal of this work \citep{Steven:2016aa}, HemeLB can scale up from 1 up to 32,000 cores in running the simulation \citep{groen2013analysing}. This means that a typical problem could run in a commodity hardware with a small number of cores. However, bigger and scientifically challenging problems will require a higher number of cores that requires high-performance computing resources as portrayed in \cite{franco2016non, franco2015dynamic} and \cite{bernabeu2015characterization}. In addition to that, users of HemeLB have to use command line interface to configure, run HemeLB simulation, and interact with the output files. 

Output files generated by this step are written in parallel into output directory which is set when running the simulation. These output files represent the state of blood flow in the vascular system at a given step count. The interval in which HemeLB writes an output is also set from the domain definition step.

\item{\textbf{Post processing}}

The output files generated by the HemeLB simulations are not easily viewed by domain experts. The files are generated in a fashion that is efficient to write in parallel, however, it will need further conversion to make it easy to be interpreted. This is where the post-processing step comes in.  

In this step, the output files are piped into two python scripts that are included in HemeLB tools to convert them into a .VTU files. These .VTU files are viewable in a separate software called ParaView. With it, domain experts could visualize the result of the simulation in a graphical user interface which ParaView provided. This step can be run on commodity hardware without problems. However, to do this step, users will require to interact with command line interface.

\end{enumerate}

All of these steps requires users to configure and install the tools required for each step by themselves. However, the target audience of HemeLB are biologists, clinician, researchers, and medical professionals, which might not have the capabilities and the technical know-how to do it. This is one of the motivating reasons for the proposed work in this thesis.


\section{High performance computing infrastructure}

Researchers increasingly use complex mathematical and computational approaches in doing their research. In understanding complex phenomenon, researchers use interdisciplinary approaches in providing insight into the problem \citep{huerta2000nih}. Bioinformatics and computational biology are examples of this interdisciplinary discipline. 

The problems that these disciplines tries to understand require computational approaches that are not cheap. The smaller size of these problems would probably run on a commodity hardware, more complex one will require highly parallel computing resources. These resources are often be found in the form of a supercomputer like ARCHER supercomputer. HemeLB software package is the prime example of these computational approach that requires highly parallel computing resources.
 

%
%The biology community is increasingly making use of mathematical and computation approaches in their research. They use these approaches to help answer questions and understand experiments in biology [12]. While small cases can run on a laptop, more complex case demand parallel computing resources like ARCHER supercomputer. HemeLB is a prime example of computational biology software that need these better computing resources.

To tackle these type of problems that require large computing resources, two paradigms of computation is developed. These are High Performance Computing (HPC) and High Throughput Computing (HTC).   While both of these disciplines are developed to solve problems that require large computing resources, they are different in the nature of the problems they are trying to solve.

High performance computing uses  uniform  computing nodes to perform a  tightly coupled computation. They are often placed in one location and connected to high bandwidth network amongst them. This network connection allows these nodes to communicate with each other efficiently, thus, allowing them to coordinate computation across the nodes\citep{Micro31:online}. A message passing interface (MPI) library is often used to perform this type of computation and it allows every process to communicate with each other in parallel. As observed in the proposal, computer clusters, GPUs, and supercomputers are the prime example of computing resources to run this type of computation.

On the other hand, High throughput computing tries to treat computing resources like a utility line. Users should not have to worry about where the computing resources come from, they can just request it and it will be given. This type of paradigm uses a middleware that allows non-uniform computing resources to communicate and cooperate in order to solve common problems\citep{Micro31:online}. These differing resources will then do different works that are scheduled independently. 

HemeLB uses MPI library to communicate between processes and run tightly coupled computations on each of these processes. Based on the definitions outlined above, we can safely categorize HemeLB as an HPC application that requires an HPC infrastructure to run its computation efficiently. These computations will simulate how the blood will flow in the blood vessel.

Running HPC application like HemeLB requires access to HPC infrastructure that is often managed by a university or a research facility. These institutions give access to HPC projects by computing hours on the basis of the merit of their proposal. For example, this is how the Partnership for Advanced Computing in Europe (PRACE)\footnote{http://www.prace-project.eu} and Engineering and Physical Science Research Council (EPSRC)\footnote{https://www.epsrc.ac.uk} operate. They conduct a peer-review of proposals that indicates the need for their infrastructure and give access selectively.

The model of operation of this institute inherently discourages reproducibility of a research. Experts with reproducibility purposes have to compete for the limited amount of available computing hours with other projects. Most likely, reproducibility of past studies is not the top priority of this institutes, causing a barrier for reproducing computational research that relies on this type of infrastructure.


In addition to the above problem, most research that tackles biological and biomedical discipline often fall outside the scope of these institutions. Also at the time of writing, the institutions on these disciplines do not have access to HPC resources. These problems limit the capabilities of researchers to reproduce past studies easily. 

%Traditionally, there are two paradigm that tackles large computing processes. These are High Performance Computing and High Throughput Computing (HTC). HPC involve using many similar computing nodes to perform tightly coupled computations. These nodes are often placed in the same room and connected with high bandwidth network. These network allow the nodes to communicate between each other in doing the computations [13]. An example for this type of resources are computer clusters, GPUs, and supercomputers. In contrast, HTC allow heterogeneous computing resources to cooperate for common goals. These resources are often distributed geographically and varies in type and performance. These resources will then do different independent computations that independently scheduled \citep{Micro31:online}. Based on these distinctions, HPC is a correct categorization of HemeLB.

%However, running these simulations requires access to HPC infrastructures that might not have reproducibility of research as a priority. Facilities that operate these infrastructure often give out computing hour usage to projects based on the merit of their peer-reviewed proposal, for example how PRACE [14], the Partnership for Advanced Computing in Europe, and EPSRC [15] give access to their infrastructure to researchers. This means that those seeking to reproduce computation of a research have to compete with other projects for the limited computing hours that are given out by these institutions. Most likely, it will not be the top priority, hence creating barrier for reproducing computational research.
%
%Furhtermore, most biology and biomedical research fall outside the remit of these organizations and their counterpart in these domains, e.g. BBSRC, HRC, do not provision HPC resources at the time of writing. Not having access to these facilities create a barrier for HemeLB to become more open because reproduction of simulation is non-trivial. This is where cloud computing infrastructure enter the picture.



\section{Cloud computing}


To answer the huge demand for computation powers by researchers and academics,  a concept called grid computing was born in the 1990s \citep{berman2003grid, foster2003grid}. This concept treats computing resources like utility grid. Computing resources should be able to be acquired without users knowing how it was provided to them. This model caters mostly to the interest of researchers and academia that usually give CPU hours based on projects proposal \citep{foster2008cloud}. An example of this is TeraGrid which ended its operation at 2011 \footnote{https://www.xsede.org/tg-archives}.

Cloud computing paradigm was then developed based on the similar idea that computing resources should be available to the users without the user knowing where it came from. However, this is where the similarity ends. Cloud computing caters more towards the business aspect of these computing utilities. While grid computing prioritizes features and functionalities that researchers and academia would like, cloud computing vendors focus on features that business will pay. Cloud computing vendors are driven by economies of scale and it will not survive if businesses do not use their service \citep{foster2008cloud}.

Conditions outlined above have created a tight feedback loop between users and the cloud vendors. This led to the development of features that users need and will pay for. As observed in the proposal, Cloud vendor now is massively scalable, allows computing resources abstraction, configurable dynamically, and provisioned on-demand. This has led cloud computing vendors to be more relevant compared to grid computing.

Cloud vendors continue to grow significantly in the recent years. In 2013, it was reported that some cloud vendors reached had more than 90\% growth per annum \citep{FSN.O51:online}. This growth enables them to keep more incentives for businesses and individuals to buy their services. In few instances \citep{AWSPr74:online, Annou90:online, Googl18:online}, cloud vendors have cut their pricing for their service and fueled more demands. Renting computing resources is getting cheaper every year and could make more sense than building your own infrastructure.

On top of that, cloud vendors also do not vet projects based on their proposals. Projects could easily get access to computing resources as long as they can pay for it. The business mindset of this cloud vendors allows reproducibility to be a priority in research, unlike requesting resources from research institutes.  This scenario is perfect for researchers and institutions that do not own their own HPC infrastructure. Instead of building their own, they can rent from the cloud vendors and does not have to worry about maintenance.

Conditions above are also capitalized by cloud vendors like Amazon by attracting customers that need computing resources for HPC applications \citep{Micro31:online}. While it is reported that running HPC applications on cloud platform will incur performance overhead, it is a viable alternative to having your own dedicated infrastructure. This is shown in various studies in the past, for example, Nekkloud project \citep{cohen2013nekkloud}, NASA HPC applications \citep{mehrotra2012performance}, and an HPC application benchmark in public cloud \citep{he2010case}. In addition to that, the capabilities to massively scale your computing resources, limited by one's purchasing power, is an attractive feat for HPC application that needs scalability. This is also why HemeLB software package can rely on the cloud platform to scale up or down depending on the problems. 



%In response to the huge demand for computational power by researchers and academics, a concept called grid computing was envisioned in 1990s [16, 17]. This vision considered computing resources analogous to power grid, where user should not care from where the resources are acquired and how it is delivered to the user. This paradigm was mainly developed with the interest of researchers and academia that the business models caters to the most [18]. Grid computing typically give CPU hours based on the proposal that is vetted by the institutions. Example of this institution is TeraGrid which operates until 2011 [19].
%
%Cloud computing shares similar vision with the grid computing paradigm, in that the computing resources are acquired and delivered are invisible to the users, but different on the execution of the business model. It is massively scalable, allows abstract encapsulation of computing resources, dynamically configured and delivered on-demand and most importantly, driven by economies of scale [18]. Since it is driven by economies of scale, it is in the interest of cloud providers to provide features that users actually need and want to pay for, therefore creating a tight feedback loop between users and the providers to develop the platform better than how grid computing handle feature developments.

%This has allowed cloud vendors to grow significantly, for example in 2013 it was noted that some cloud vendors could reach more than 90\% growth per annum [20]. This growth further fuels demand and allow them to cut pricing for their service multiple times [21, 22, 23] and create more demands. This development has allowed businesses and institutions to offload their computational need to the cloud vendors for a price rather than building their own infrastructure. This scenario could also be used for our purpose of performing or reproducing computational research without needing to have access to large HPC systems.
%
%Cloud vendors like Amazon also capitalize on the need for computing resources for HPC applications [13]. Running HPC applications on cloud platforms, while incurring performance overhead, can be a viable alternative to supercomputers as shown by the Nekkloud project [24], NASA HPC Applications [25], and HPC applications benchmark in cloud case study [26]. Also, part of this project is to demonstrate that HemeLB can run acceptably on a cloud platform.



\section{Other High Performance Computing}

In the past few years, many complex HPC software packages have been deployed to the cloud. In this section, I will highlight these projects to learn how they solve similar issues.

One similar project is Nekkloud [24]. In this case, Nektar++, a complex high-order finite element code, face similar usability problems. Their original workflow was so complex that only few people can run it. People without computer expertise had a hard time to actually run computations with it. Furthermore, one should also get access to a HPC infrastructure to run it, which may not be easy. Nekkloud project is their answer to these problems. It was developed to encapsulate most difficulties in using the software package. Using a web application to provide high level interface instead of using the command line. Making it more accessible to more people without computing expertise. In addition to that, it ran on cloud infrastructures. Allowing people without dedicated HPC infrastructure to run high-order finite element computations.

Another project that is tackling similar space is Galaxy [27]. Galaxy, a web-based reproducible research platform, uses cloud infrastructure to run its HPC applications. In illustrating its use, the developers have developed a super-resolution spark (SRS) model. This modeling process needs a supercomputing resources to execute the cloud infrastructure provides. These capabilities are also encapsulated in an easy to use web interfaces, making it easy for scientists to run, and share simulations.

Above examples illustrate that a web application can be a viable alternative interface for complex applications. However, this implementation on the cloud also has a negative impact on the applications. Raw performance is lower than dedicated HPC infrastructures. These performance penalty was observed in the projects mentioned already [24, 25, 26]. Nekkloud authors considered the performance penalty acceptable, because the cloud infrastructures allow flexibility. This flexibility and the benefit of making it more usable will sometimes outweigh the performance penalty.

Pros and cons of web application for complex HPC projects are area that are often discussed. But, deployment scenario for these HPC projects in cloud infrastructure are rarely discussed. More specifically, the use of containerization technology in helping tools deployment.

Deploying HPC applications is considered as a time-intensive process [28]. For example, the ARCHER support team has 36 members [29] to support this process. One approach to reduce these problems is software containerization. Containerization technology is developed to run applications or tools in an isolated environment within a kernel. It is more lightweight than traditional virtualization technology that use hypervisors to manage virtual machines [30]. Containerization technology has been discussed in high performance computing area. For example how Docker, one of the more popular implementation of containerization technology, is abstracting software environment in the HPC infrastructure [31] and used to build virtual HPC clusters [32]. Also, the shifter project [33] is trying to unleash Docker on HPC infrastructure. Meaning, allowing their HPC infrastructure to use Docker capabilities. To date, I am not aware of any discussion on the effect of containerization in running HPC application in cloud.

One of the above projects, Galaxy, support containerization technology for their tools packaging. They used Docker, one implementation of linux container software. Galaxy claimed that using Docker allow efficiency, isolation, and portability of their tools [34]. These are good traits that could also be helpful for HemeLB. Docker, in particular, are often discussed as a promising technology to support reproducible research [35]. Usage of containerization technology, however, are sparsely detailed in the literature.


\section{Reproducibility problem in computational research}


\section{Containerization Technology}



