% Activate the following line by filling in the right side. If for example the name of the root file is Main.tex, write
% "...root = Main.tex" if the chapter file is in the same directory, and "...root = ../Main.tex" if the chapter is in a subdirectory.
 
%!TEX root =  dissertation.tex

\chapter[Background]{Background}

In this chapter, I will discuss various background information that is used as a basis for the work presented in this dissertation. I will discuss how HemeLB currently works, the High-Performance Computing (HPC) infrastructure, and Docker.


\section{Usability of complex applications}

\subsection{Current HemeLB workflow}

Currently, running a blood flow simulation with HemeLB consists of multiple steps that need to be run in sequence. These steps are followed in a variety of interfaces, from the Command Line Interface (CLI) to Graphical User Interface (GUI). Additionally, these steps also require various levels of computing resources to work efficiently. In order to understand how the proposed work will improve the current conditions, I will elaborate on how HemeLB currently works.

%Running a blood flow simulation using HemeLB currently consists of multiple steps. To understand how the proposed project can improve the current conditions, I will elaborate on how HemeLB workflow currently work based on the work on my proposal\cite{Steven:2016aa}.



\vspace{1cm}

\noindent%
\begin{minipage}{\linewidth}% to keep image and caption on one page
\makebox[\linewidth]{
  \includegraphics[keepaspectratio=true,scale=0.6]{../resources/images/HemeLB-workflow.png}
 }
\captionof{figure}{Current HemeLB workflow taken from HemeWeb proposal \citep{Steven:2016aa}}\label{fig:hemelb-workflow}%      only if needed  
\end{minipage}

\vspace{1cm}


Figure \ref{fig:hemelb-workflow} illustrates the steps involved in running the HemeLB workflow. These steps will be discussed in detail below:

\begin{enumerate}

\item{\textbf{Geometrical model reconstruction}}

In this step, a 3D model of a vascular system is constructed from the raw microscopy image of it. Alternatively, the 3D model can also be constructed from a CT scan with its 3D imaging data. From this step, a 3D geometry file is generated in the form of a .STL (STereoLithography) file,  a standard file format to describe 3D model object geometry. This  process can run in a regular workstation. However, it is highly problem dependent as the tools needed to parse and generate the 3D model are dependent on the problem domain experts try to simulate.

\item{\textbf{Domain definition}}

The 3D geometry model generated from the previous step is now used as input for the domain definition step. In this step, a graphical user interface is used to add domain information to the 3D model. Information like blood viscosity, inlet outlet placement, and blood pressure will determine how the simulation will run. The HemeLB setup tool was developed for this particular need. The setup tool provides a graphical user interface for domain experts to add these parameters. All parameters are then saved in a profile file in .pr2 format. This step can run on standalone commodity hardware and should not require highly parallel computing resources.

\item{\textbf{Geometry generation}}

This step will take the encoded information from the domain definition step and the 3D model of the vascular system to generate files that can be understood by the main HemeLB program. These files contain similar information as the previous 3D model and the profile file. However, both of them are now formatted in a HemeLB parseable format, an XML configuration file and a GMY geometry file. This geometry generation step can also run on a commodity hardware. However, it requires users to use the CLI to convert the files. The process is done by piping the input files into a Python script which is part of the HemeLB setup tools. In scientific computing this step is also known as domain discretisation.

\item{\textbf{HemeLB simulation}}

The main heavy computations of the workflow are done in this step.  Configuration and geometry files that are generated in the previous step are feed into the HemeLB binary as input files. HemeLB will then run calculations that govern how blood will flow inside the provided vascular system for a number of iterative steps. The number of steps is defined in the configuration file that is generated in the domain definition steps. 

As observed in the proposal of this work \citep{Steven:2016aa}, HemeLB can scale up from 1 up to 32,000 cores in running the simulation \citep{groen2013analysing}. This means that a typical problem could run in a commodity hardware with a small number of cores. However, bigger and scientifically challenging problems will require a higher number of cores that requires high-performance computing resources as portrayed in studies by Franco et al. \cite{franco2016non, franco2015dynamic} and Bernabeu et al. \cite{bernabeu2015characterization}. In addition to that, users of HemeLB have to use the CLI to configure, to run the HemeLB simulation, and to interact with the output files. 

The output files generated by this step are written in parallel into the output directory which is set when running the simulation. These output files represent the state of blood flow in the vascular system at a given step count. The interval in which HemeLB writes an output is also set from the domain definition step.

\item{\textbf{Post processing}}

The output files generated by the HemeLB simulations are not easily viewed by domain experts. The files are generated in a fashion that is efficient to write in parallel, however, it will need further conversion to make it easy to interpret. This is where the post-processing step comes in.  

In this step, the output files are piped into two python scripts that are included in HemeLB tools to convert them into .VTU files. These .VTU files are viewable in separate software called ParaView \footnote{\url{http://www.paraview.org}}. With it, domain experts can visualize the result of the simulation in a graphical user interface which ParaView provided. This step can be run on commodity hardware without any problems. However, to do this step, users will need to interact with the command line interface.

\end{enumerate}

All of these steps require users to configure and install the tools required for each step by themselves. However, the target audience of HemeLB are biologists, clinicians, and researchers, who might not have the capabilities and the technical know-how to do it. This is one of the motivating reasons for the proposed work in this thesis.


\subsection{High performance computing infrastructure}

Researchers increasingly use complex mathematical and computational approaches in doing their research. In understanding complex phenomenon, researchers use interdisciplinary approaches to provide insight into the problem \citep{huerta2000nih}. Bioinformatics and computational biology are examples of this interdisciplinary fields. 

The problems that these disciplines try to understand require computational approaches that are costly. The smaller sized problems would probably run on a a powerful workstation, while more complex ones will often require institutional or regional HPC facility. These resources are often found in the form of a supercomputer like the ARCHER supercomputer. The HemeLB software package is one of many examples of these computational approaches that require highly parallel computing resources.
 

%
%The biology community is increasingly making use of mathematical and computation approaches in their research. They use these approaches to help answer questions and understand experiments in biology [12]. While small cases can run on a laptop, more complex case demand parallel computing resources like ARCHER supercomputer. HemeLB is a prime example of computational biology software that need these better computing resources.

To tackle problems that require large computing resources, two paradigms of computation are developed. These are High Performance Computing (HPC) and High Throughput Computing (HTC).   While both of these disciplines are developed to solve problems that require large computing resources, they are different in the nature of the problems they are trying to solve.

High performance computing uses  uniform  computing nodes to perform a  tightly coupled computation. They are placed in one physical location and are connected to a high bandwidth network amongst them. This network connection allows these nodes to communicate with each other efficiently, thus, allowing them to coordinate computation across the nodes \citep{Micro31:online}. A message passing interface (MPI) library is often used to perform this type of computation and it allows every process to communicate with each other in parallel. As observed in the proposal, computer clusters, GPUs, and supercomputers are prime examples of computing resources to run this type of computation.

On the other hand, high throughput computing tries to treat computing resources like a utility line. Users should not have to worry about where the computing resources come from, they can just request them and they will be provided. This type of paradigm uses a middleware that allows non-uniform computing resources to communicate and cooperate in order to solve common problems \citep{Micro31:online}. These differing resources will then do different works that are scheduled independently. 

HemeLB uses the MPI library to communicate between processes and runs tightly coupled computations on each of these processes. Based on the definitions outlined above, we can categorize HemeLB as an HPC application that requires an HPC infrastructure to run its computation efficiently. 

Running an HPC application like HemeLB requires access to an HPC infrastructure which is often managed by a university or a research facility. These institutions give access to HPC projects by computing hours on the basis of the merit of their proposal. For example, this is how the Partnership for Advanced Computing in Europe (PRACE)\footnote{http://www.prace-project.eu} and the Engineering and Physical Science Research Council (EPSRC)\footnote{https://www.epsrc.ac.uk} operate. They conduct a peer-review of proposals that indicate the need for their infrastructure and give access selectively.

The model of operation of this institute often does not prioritize the reproducibility of a research study or an exploratory one. Experts wishing to reproduce previous simulation have to compete for the limited amount of available computing hours with other projects. Most likely, the reproducibility of past studies is not the top priority of these institutes, causing a barrier for reproducing computational research that relies on this type of infrastructure.


In addition to the above problem, most research that tackles biological and biomedical disciplines often falls outside the scope of these institutions. Also at the time of writing, the equivalent funding bodies for these disciplines do not have access to HPC resources. These problems limit the capabilities of researchers to reproduce past studies easily. 

%Traditionally, there are two paradigm that tackles large computing processes. These are High Performance Computing and High Throughput Computing (HTC). HPC involve using many similar computing nodes to perform tightly coupled computations. These nodes are often placed in the same room and connected with high bandwidth network. These network allow the nodes to communicate between each other in doing the computations [13]. An example for this type of resources are computer clusters, GPUs, and supercomputers. In contrast, HTC allow heterogeneous computing resources to cooperate for common goals. These resources are often distributed geographically and varies in type and performance. These resources will then do different independent computations that independently scheduled \citep{Micro31:online}. Based on these distinctions, HPC is a correct categorization of HemeLB.

%However, running these simulations requires access to HPC infrastructures that might not have reproducibility of research as a priority. Facilities that operate these infrastructure often give out computing hour usage to projects based on the merit of their peer-reviewed proposal, for example how PRACE [14], the Partnership for Advanced Computing in Europe, and EPSRC [15] give access to their infrastructure to researchers. This means that those seeking to reproduce computation of a research have to compete with other projects for the limited computing hours that are given out by these institutions. Most likely, it will not be the top priority, hence creating barrier for reproducing computational research.
%
%Furhtermore, most biology and biomedical research fall outside the remit of these organizations and their counterpart in these domains, e.g. BBSRC, HRC, do not provision HPC resources at the time of writing. Not having access to these facilities create a barrier for HemeLB to become more open because reproduction of simulation is non-trivial. This is where cloud computing infrastructure enter the picture.



\subsection{Cloud computing}


To answer the huge demand for computation powers from researchers and academics,  a concept called grid computing was born in the 1990s \citep{berman2003grid, foster2003grid}, in which computing resources are treated like utility grid. Computing resources should be able to be acquired without users knowing how they were provided to them. This model caters mostly to the interest of researchers and academia that usually give CPU hours based on projects proposal \citep{foster2008cloud}. As an example of this is the TeraGrid\footnote{https://www.xsede.org/tg-archives} project which ended its operations in 2011.

The cloud computing paradigm was then developed by commercial operators based on a similar idea that computing resources should be available to users without them knowing where it came from. However, this is where the similarity ends. Cloud computing caters more to the business aspect of these computing utilities. While grid computing prioritizes the features and functionalities that researchers and academia would like, cloud computing vendors focus on features that business will pay for. Cloud computing vendors are driven by economies of scale and it will not survive if businesses do not use their services \citep{foster2008cloud}.

The conditions outlined above have created a tight feedback loop between users and the cloud vendors. This led to the development of features that users need and will pay for. As observed in the proposal, cloud vendors now are massively scalable, allows computing resources abstraction, configurable dynamically, and provisioned on-demand. This has led to cloud computing vendors being more relevant to common use-cases compared to grid computing.

Cloud vendors continue to grow significantly. In 2013, it was reported that some cloud vendors had a more than 90\% growth per annum \citep{FSN.O51:online}. This growth enables them to have more incentives for businesses and individuals to buy their services. In a few instances \citep{AWSPr74:online, Annou90:online, Googl18:online}, cloud vendors have cut their pricing for their services and reduced price fueled more demands. Renting computing resources is getting cheaper every year and could make more sense both economically and technically than building your own infrastructure.

On top of that, cloud vendors also do not vet projects based on their proposals. Projects could easily start on-demand computing units for their purpose. The business mindset of these cloud vendors allows reproducibility to be a priority in research, unlike requesting resources from research institutes.  This scenario is perfect for researchers and institutions that do not own their own HPC infrastructure. Instead of building their own, they can rent them from the cloud vendors and not have to worry about maintenance.

The above conditions are also capitalized by cloud vendors like Amazon by attracting customers that need computing resources for HPC applications \citep{Micro31:online}. While it is reported that running HPC applications on a cloud platform will incur performance overheads, it is a viable alternative to having your own dedicated infrastructure. This is shown in various studies in the past, for example, the Nekkloud project \citep{cohen2013nekkloud}, NASA HPC applications \citep{mehrotra2012performance}, and an HPC application benchmark in the public cloud \citep{he2010case}. In addition, the capabilities to massively scale your computing resources, limited by one's purchasing power, is an attractive feat for an HPC application that needs scalability. This is also why the HemeLB software package can rely on the cloud platform to scale up or down depending on the problems. 



%In response to the huge demand for computational power by researchers and academics, a concept called grid computing was envisioned in 1990s [16, 17]. This vision considered computing resources analogous to power grid, where user should not care from where the resources are acquired and how it is delivered to the user. This paradigm was mainly developed with the interest of researchers and academia that the business models caters to the most [18]. Grid computing typically give CPU hours based on the proposal that is vetted by the institutions. Example of this institution is TeraGrid which operates until 2011 [19].
%
%Cloud computing shares similar vision with the grid computing paradigm, in that the computing resources are acquired and delivered are invisible to the users, but different on the execution of the business model. It is massively scalable, allows abstract encapsulation of computing resources, dynamically configured and delivered on-demand and most importantly, driven by economies of scale [18]. Since it is driven by economies of scale, it is in the interest of cloud providers to provide features that users actually need and want to pay for, therefore creating a tight feedback loop between users and the providers to develop the platform better than how grid computing handle feature developments.

%This has allowed cloud vendors to grow significantly, for example in 2013 it was noted that some cloud vendors could reach more than 90\% growth per annum [20]. This growth further fuels demand and allow them to cut pricing for their service multiple times [21, 22, 23] and create more demands. This development has allowed businesses and institutions to offload their computational need to the cloud vendors for a price rather than building their own infrastructure. This scenario could also be used for our purpose of performing or reproducing computational research without needing to have access to large HPC systems.
%
%Cloud vendors like Amazon also capitalize on the need for computing resources for HPC applications [13]. Running HPC applications on cloud platforms, while incurring performance overhead, can be a viable alternative to supercomputers as shown by the Nekkloud project [24], NASA HPC Applications [25], and HPC applications benchmark in cloud case study [26]. Also, part of this project is to demonstrate that HemeLB can run acceptably on a cloud platform.



\subsection{How other HPC applications tackle usability issues}

In this section, I will discuss how similar HPC applications solve problems like HemeLB faces. Recently, some complex HPC software packages have been deployed to the cloud. Their experience in deploying the software and their approach to solving similar problems will be indispensable for the work I proposed.

The first project that tackles a similar problem to HemeLB is Nekkloud. In this project, Nektar++ faces a usability issue just like HemeLB. It is a complex high-order finite element code which is mainly operated using command line interface. The original workflow is complex and only a limited number of people can operate it, barring people without computer expertise from actually taking advantage of the software package.

This usability problem coupled with the fact that users have to get access to the HPC infrastructure might not be a viable option for everyone. In order to answer these problems, the Nekkloud project was hatched. Nekkloud was developed to make the software configuration invisible to users. It uses a web application to provide an easy to use and learn interface for the domain experts. This allows people without computing expertise to actually run Nektar++ without having to be troubled with configurations. In addition to solving the usability problems, Nekkloud also uses public cloud vendors, alleviating concerns with regards to having dedicated HPC infrastructures from users. 

Galaxy \citep{goecks2010galaxy} is another project that is trying to tackle similar problems. It describes itself as a web-based reproducible research platform and runs on public cloud infrastructure. With Galaxy, researchers can run various HPC applications that are compatible with an easy to use web interface. Users do not have to worry about the configurations and working in command lines, only about the software executions.

The developers of Galaxy developed a super-resolution spark (SRS) model to illustrate its use case. This model is an example of an HPC application that requires highly parallel computing resources like a supercomputer to run efficiently. However, running the SRS model in the cloud via Galaxy is observed to be a viable alternative. In addition to that, Galaxy provides an easy to use interface to run this model for users, making it easy for them to run their experiments and share them with their peers.

%
%In the past few years, many complex HPC software packages have been deployed to the cloud. In this section, I will highlight these projects to learn how they solve similar issues.
%
%One similar project is Nekkloud [24]. In this case, Nektar++, a complex high-order finite element code, face similar usability problems. Their original workflow was so complex that only few people can run it. People without computer expertise had a hard time to actually run computations with it. Furthermore, one should also get access to a HPC infrastructure to run it, which may not be easy. Nekkloud project is their answer to these problems. It was developed to encapsulate most difficulties in using the software package. Using a web application to provide high level interface instead of using the command line. Making it more accessible to more people without computing expertise. In addition to that, it ran on cloud infrastructures. Allowing people without dedicated HPC infrastructure to run high-order finite element computations.
%
%Another project that is tackling similar space is Galaxy [27]. Galaxy, a web-based reproducible research platform, uses cloud infrastructure to run its HPC applications. In illustrating its use, the developers have developed a super-resolution spark (SRS) model. This modeling process needs a supercomputing resources to execute the cloud infrastructure provides. These capabilities are also encapsulated in an easy to use web interfaces, making it easy for scientists to run, and share simulations.

Nekkloud and Galaxy projects illustrate that a web interface is a viable alternative interface for complex HPC applications. With the correct application and design, it will allow domain experts to operate the underlying HPC application with relative ease. However,  the change of underlying infrastructure from a dedicated HPC infrastructure to the public cloud has some negative impacts.

One notable drawback is the lower performance of the HPC applications. It has been studied that running HPC applications on the public cloud means performance degradation compared to the dedicated HPC infrastructure. Mehrotra et al. \citep{mehrotra2012performance} conducted a performance evaluation of the NASA HPC applications on Amazon EC2 and found out that in the worst case using Amazon EC2 as an HPC infrastructure increases runtime by up to a factor of five. The worst case is consistently found in the larger number of cores used, compared to the smaller number of cores. This severe penalty is largely caused by the inherent difference in the networking capabilities between cloud vendors like Amazon with supercomputers, as the more nodes the more severe the penalty is.  Amazon Web Service offers 10GigE (10Gb/s) compared to FDR InfiniBand (54.5Gb/s) on the NASA supercomputer, Pleiades. In addition, Mehrotra et al. also found that there was a small performance penalty on Amazon due to its virtualization layer. This  penalty should be carefully considered when running HemeLB in the cloud.

%Above examples illustrate that a web application can be a viable alternative interface for complex applications. However, this implementation on the cloud also has a negative impact on the applications. Raw performance is lower than dedicated HPC infrastructures. These performance penalty was observed in the projects mentioned already [24, 25, 26]. Nekkloud authors considered the performance penalty acceptable, because the cloud infrastructures allow flexibility. This flexibility and the benefit of making it more usable will sometimes outweigh the performance penalty.
%
%Pros and cons of web application for complex HPC projects are area that are often discussed. But, deployment scenario for these HPC projects in cloud infrastructure are rarely discussed. More specifically, the use of containerization technology in helping tools deployment.
%
%Deploying HPC applications is considered as a time-intensive process [28]. For example, the ARCHER support team has 36 members [29] to support this process. One approach to reduce these problems is software containerization. Containerization technology is developed to run applications or tools in an isolated environment within a kernel. It is more lightweight than traditional virtualization technology that use hypervisors to manage virtual machines [30]. Containerization technology has been discussed in high performance computing area. For example how Docker, one of the more popular implementation of containerization technology, is abstracting software environment in the HPC infrastructure [31] and used to build virtual HPC clusters [32]. Also, the shifter project [33] is trying to unleash Docker on HPC infrastructure. Meaning, allowing their HPC infrastructure to use Docker capabilities. To date, I am not aware of any discussion on the effect of containerization in running HPC application in cloud.
%
%One of the above projects, Galaxy, support containerization technology for their tools packaging. They used Docker, one implementation of linux container software. Galaxy claimed that using Docker allow efficiency, isolation, and portability of their tools [34]. These are good traits that could also be helpful for HemeLB. Docker, in particular, are often discussed as a promising technology to support reproducible research [35]. Usage of containerization technology, however, are sparsely detailed in the literature.

\section{Reproducibility of software}

In this section, I will discuss the problem of reproducibility and how the proposed components try to solve it

\subsection{Reproducibility problem in computational research}

The American Physical Society \citep{APS:aa} describes science as "the systematic enterprise of gathering knowledge about the universe and organizing and condensing that knowledge into testable laws and theories". For theories and experiments to be testable, they have to be independently reproducible or replicable by peers. However, a recent study \citep{open2015estimating} highlights  that some published psychology studies are not replicable to the same significance as reported. At machine learning conferences \citep{drummond2009replicability}, a similar sentiment is being shared. Not being replicable does not necessarily means that the result of those studies is wrong. However, it suggests that academias may not prioritize the verification of results. The novelty of an idea may be seen as better than verifying what we already know.

With the recent study highlighting a reproducibility problem of a research study, previous pushes \citep{donoho2010invitation, sandve2013ten} for reproducibility in studies become more relevant than ever. This is especially crucial in a discipline like computational research. Computational research disciplines like bioinformatics and computational physics involve complex computations that requires huge computational power depending on the problem size. The complexity of the configuration, algorithm, and execution process actually become a barrier for peers to replicate and reproduce the works of studies on this discipline. Making results produced have less weight than it could be

This complexity is also one of the reasons why the Galaxy project came into development \citep{goecks2010galaxy}. While science values rigorous testing, replicable, and reproducible results, the complexity of the HPC software package hinders that. Galaxy tries to solve that by producing automatic metadata that record the execution of an analysis done by tools on the Galaxy platform. It records all the input files, configurations, and outputs of an analysis and makes them available for the users to view and copy. This metadata information enables users to share the analysis with their peers, allowing peers to replicate or reproduce the analysis results independent of the original researcher. Galaxy allows ease of reproducibility and replicability for the domain experts. 



\subsection{Containerization technology and HPC application}




One of the main problems with replicating or even reproducing the results of studies with a complex software package is configurations. A complex software package like typical HPC applications often requires hands-on configuration by users for it to run correctly. This complex configuration coupled with complex usage becomes a barrier for an independent party to verify the results of a study.

Containerization technology, particularly Docker\footnote{https://www.docker.com/}, can help with the issue above. Docker is a technology that is developed on top of kernel-level technologies like Linux Containers (LXC)\footnote{\url{https://linuxcontainers.org}} which allow containers to be a sandbox between processes \citep{merkel2014docker}. Containers allow applications to be securely placed in their own environment that shares the kernel with its host. Unlike full virtualization, containerization does not require a full operating system to be installed on the virtual environment so the application can run. Docker is arguably the most popular containerization technology currently. It is built on LXC and added features that allow an application to be deployed easily. It handles container image creation, versioning, sharing and archiving in addition to running the image like LXC does. On top of that, there is also a web application called Docker hub\footnote{https://hub.docker.com} that allows Dockerfile, a text file that contains the commands to build a container, to be inspected publicly if set correctly. It allows interested peers to audit a container and to make changes to it for their own container.

Using containerization technology like Docker can help with reproducing the results of studies. A complex configuration process can be recorded and encapsulated in the form of Docker container;  hiding complexity to the casual users. Studies can package the complete software package in a Docker container to be shared with peers for independent replication and reproduction of an analysis. In addition to that, the versioning feature of Docker also allows the software package to be continuously developed and for an analysis to be tied down to an older version of the package. As an analysis can be replicated even if the software is currently way ahead compared to the time an analysis is made. It  also creates a good opportunity for the study to be reproduced, an analysis could have different results if it is running with the new version of the package. Hence, containerization technology enables reproducibility of results.

This is also the reason why the Galaxy project supports Docker in its toolsets. The galaxy ecosystem uses Docker to create a secure, isolated environment for the tools and its dependencies \citep{moreews2015curated}. Tools are versioned, archived, and shared with Docker containers so peers can download and audit all the tools. This openness is also important for the tools to be trusted.

HemeLB as a complex HPC application is also actively developed. While development is ongoing, it is often used as part of a research study. Docker can help with the recording of the version used for a study with its versioning feature. In addition to that, configuration work is also taken out of the hands of the peer because it is done initially during the packaging of the application. Containerization technology sounds appropriate to be used for HemeWeb to enable easy reproduction, replication, and audit for HemeLB simulation.






%Another problem in reproducing or replicating complex studies, especially in computational research discipline is configurations. Complex software package required for said HPC application to run often requires computer expertise for it to be set up correctly. However, most domain experts that use the HPC application often does not have the required expertise, making it a barrier for them to reproduce a study.
%
%Containerization technology is something that can be used to address that problem.  Container technology use a linux LXC container, that allows a separate environment to be created. This environment is contained in a container that can be easily shared and moved. Users usually configure the container with the software package they are interested in reproducing in other hardware and share them with their peers. This take the work of configuration out of the peers.
%
%Docker is one of the most popular and has the most easy to use interface. Not only for creating containers, Docker also have their own web application counterpart that enables user to list, upload, and download container easily. With the social aspect of the web application, using Docker is quite easy.
%
%Galaxy, also is documented to support containerization technology. They use Docker to containerized the tools they support on the web application. It is argued that Docker has allowed efficiency, isolation, and portability of their tools.  These traits have made Docker a good tools to support reproducibility of a computation research. However, there are

%One of the above projects, Galaxy, support containerization technology for their tools packaging. They used Docker, one implementation of linux container software. Galaxy claimed that using Docker allow efficiency, isolation, and portability of their tools [34]. These are good traits that could also be helpful for HemeLB. Docker, in particular, are often discussed as a promising technology to support reproducible research [35]. Usage of containerization technology, however, are sparsely detailed in the literature.

%HemeLB as described before has some configuration complexity. It is a prime candidate to be distributed to the peer using Docker.




