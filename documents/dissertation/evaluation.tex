% Activate the following line by filling in the right side. If for example the name of the root file is Main.tex, write
% "...root = Main.tex" if the chapter file is in the same directory, and "...root = ../Main.tex" if the chapter is in a subdirectory.
 
%!TEX root =  dissertation.tex

\chapter[Evaluation]{Evaluation}
In this chapter, I will discuss on the system's evaluation. Success will be measured by answering these questions:

\begin{itemize}
    \item Can users run a simulation using our system ?
    \item Can users reproduce past simulations using our system ?
    \item Are users satisfied in using our system ?
    \item Are users more likely to run a simulation using our system compared to the command line ?
    \item How does it perform compared to the existing infrastructure ?
\end{itemize}


In answering questions above, I conduct two sets of evaluations. First, an online questionnaire was conducted to measure user's experience and system's usability. And secondly, a performance analysis comparing the performance of HemeLB between dedicated hardware against cloud vendors. The questionnaire will answer most of the questions related to user experience and usability above, while the performance benchmark will be the basis for performance evaluation and justification in using HemeWeb.

%These questions will be answered in two different evaluations. First, an online questionnaire was sent to the target audience of HemeWeb. In the questionnaire, testers will be asked to run two scenarios and answer questions to measure their experience. These answers will be the basis for answering most of the questions above. In addition to the questionnaire, I will measure the performance of HemeWeb in running HemeLB simulation by running benchmarks and comparing the raw performance number. These benchmarks will be the basis of the performance measure of the evaluation.

\section{Questionnaire}

In measuring user's experience, I created a questionnaire for HemeWeb using google form. The questionnaire was live for 10 days, from 3rd of August 2016 until 12th of August 2016.

%I created a questionnaire that will measure user's experience in using HemeWeb. This questionnaire was created and shared using google forms. It ran online for 10 days from 1st August 2016 - 10th August 2016.

At the start of the questionnaire, respondents are asked about their background information. This information will be used to provide demographic insight on the respondents and how it affect the type of answers the respondents will most likely give. Respondents will be asked to fill out their age, gender, job, discipline, and level of familiarity with various software tools. From their responses, I can determine whether the sample population is representative of the target audience.

%In the questionnaire, testers are asked to answer about their background information to provide demographic insight on them. Testers are asked to fill in their background information like age, gender, job, discipline. In addition, this part also ask about testers' familiarity with browsers, installing software from source code, and computational fluid dynamic software. These are required for us to determine whether our sample population is representative of the target audience of the application developed.

The questionnaire is set to make tester run specific scenarios that HemeWeb is developed for. These scenarios are running a simulation using the web interface and reproducing past simulation with it. Testers running the scenario are given an option to skip the scenario and go straight to the questions below it to measure their experience if they find it too difficult. 

The first scenario asks testers to run a simulation with given inputs. The questionnaires list two input files, a geometry file and a HemeLB configuration file that testers will need to download to their computers. After downloading the input files, testers are asked to open their browser and go to a specific URL where HemeWeb was deployed for the evaluation purposes. In that URL, testers will then add a new simulation job with the downloaded files, configure the job, and submit the job to the queue. The scenario ends when the job is submitted.

The second scenario asks testers to reproduce past simulation with a given URL that contains past simulation files. It asks testers to create a new job from the given URL instead of using files that are downloaded in the past scenario. After creating a new job, testers then will change some configuration and parameters from the past scenario and submit the job. The scenario also ends when the job is submitted.

Following each scenario are questions to measure whether users skip the scenario and an after-scenario questionnaires that users have to answer. The after-scenario questionnaire is based on the usability measurement at IBM developed by James R Lewis \citep{lewis1995ibm}.  It measures users' usability satisfaction with the system with regards to given scenario. The questionnaire give 3 statements which testers should agree or disagree, they are:

\begin{itemize}
	\item Overall, I am satisfied with the ease of completing the tasks in this scenario
	\item Overall, I am satisfied with the amount of time it took to complete the tasks in this scenario
	\item Overall, I am satisfied with the support information (online-line help, messages, documentation) when completing the tasks
%	\item If any, give suggestions on what could be better
\end{itemize}

In addition to above questions, there are some questions about the tester's willingness to do exactly the same tasks as the scenario, but with the command line. This question will measure user's willingness in using the command line interface compared to the web interface. 


After running both scenarios, testers are then redirected to the final questionnaire. The Post Study System Usability Questionnaire which is based on the same work by James R Lewis \citep{lewis1995ibm}. In this questionnaire, testers are given 19 statements where they should agree or disagree, they are:
\begin{itemize}
	\item Overall, I am satisfied with how easy it is to use this system
	\item It was simple to use this system
	\item I can effectively complete my work using this system 
	\item I am able to complete my work quickly using this system
	\item I am able to efficiently complete my work using this system
	\item I feel comfortable using this system
	\item It was easy to learn to use this system
	\item I believe I became productive quickly using this system
	\item The system gives error messages that clearly tell me how to fix problems
	\item Whenever I make a mistake using the system, I recover easily and quickly
	\item The information (such as online help, on-screen messages, and other documentation) provided with this system is clear
	\item It is easy to find the information I needed
	\item The information provided for the system is easy to understand
	\item The information is effective in helping me complete the tasks and scenarios 
	\item The organization of information on the system screens is clear
	\item The interface of this system is pleasant
	\item I like using the interface of this system
	\item This system has all the functions and capabilities I expect it to have 
	\item Overall, I am satisfied with this system
\end{itemize}

In addition to the above questions, testers also will be asked to list the most negative aspects and positive aspects of the system if they have any. These questions will measure users satisfaction with the overall system, whether it is useful, whether the information given by the system is any good, and the interface quality.

%There are 4 sections of the questionnaires. The first one is to capture the demographic of the testers. Testers are asked to fill in their background information like age, gender, job, discipline. In addition, this part also ask about testers' familiarity with browser, installing software from source code, and computational fluid dynamic software.

\section{Performance benchmarks}

The second part of the evaluation is the performance benchmark. HemeWeb is running HemeLB simulation outside its original scope of being used on a highly parallel computing resources like a supercomputer. This could have an interesting impact on the performance of the system because cloud vendors, while being easily accessible and provisioned, have an underlying infrastructure difference with the stand-alone infrastructure. This performance benchmarks will then measure whether the impact on the performance justify the claimed usability benefit that we measure on the first half of the evaluation.

The performance benchmark will be done by the internal tooling that is available inside HemeLB. Every HemeLB simulation will produce a report file that measures the performance of said simulation. On this evaluation, I will compare the performance of HemeLB simulation on three different infrastructure to measure the performance impact on having HemeLB simulation on the cloud with Docker containers. 

They are:
\begin{itemize}
	\item{ARCHER supercomputer}
	\item{INDY2 HPC Cluster}
	\item{Cloud computing infrastructure with AWS EC2}
\end{itemize}

On these three infrastructure,  ARCHER supercomputer has the oldest processor architecture. It use a three-year old 2.7 GHz, 12-core E5-2697 v2 Ivy Bridge processor as the basis of its compute node. Each compute node consist of this two processors. INDY2 is a newly installed HPC infrastructure on EPCC that is currently in early-access testing. It has the newer Broadwell-based 18-core Intel Xeon CPU E5-2695 v4 @ 2.10GHz. Lastly, for AWS-EC2, we use Amazon's c4.8xlarge EC2 instance which has Haswell-based E5-2666 v3 processor that has 18 virtual cores\footnote{\url{https://aws.amazon.com/ec2/virtualcores/}} or 36 vCPU with hyperthreading\footnote{\url{https://aws.amazon.com/ec2/instance-types/}}. Essentially, we will be using 18 core on AWS-EC2 instance for the simulation, because HemeLB is a compute bound task that do not benefit from hyperthreading. 

Next, the three different architecture also have different networking interface that could contribute to the performance difference. While AWS-EC2 offer 10 Gigabit per second interface, ARCHER use a Cray Aries router interconnectivity that provides higher network throughput (ranging from 12.5 Gigabit per second to 14 Gigabit per second) throughout the infrastructure. Finally, INDY2 has FDR InfiniBand network interface connected to each compute node that provides 54.5 Gigabit per second. 


For the simulation, we used an input file which has 4,520,681 fluid sites\footnote{Available online at \url{https://github.com/SeiryuZ/HemeWeb/blob/master/deployment/roles/hemeweb_master/files/990_Example2-skeleton_corrected_tubed_smoothed.gmy}}  \footnote{Config for the simulation can be found online at \url{https://github.com/SeiryuZ/HemeWeb/blob/master/documents/resources/evaluation/performance/config.xml}}. With these files, we are going to do a strong scaling analysis, where we keep the problem size the same while we increase the core count. According to the performance analysis by Groen et al. \citep{groen2013analysing}, HemeLB scales up near-linearly up to 32,768 cores. It also performs near its maximum efficiency when using 5,000 to 500,000 sites per core. This means that for the problem size we use, HemeLB should perform near maximum efficiency when using 9 cores up to 900 cores. Above that core counts, HemeLB simulation will incur a performance penalty.


The benchmark on ARCHER supercomputer will be the gold standard of the performance. It's a gold standard because the infrastructure has a clear purpose of being used for the HPC application. It has the necessary resources and components that are tailored for it. It should be the ideal performance scenario. Next, the INDY2 as the locally installed and managed HPC infrastructure. INDY2's performance will paints a picture if how HemeLB will perform on a private local infrastructure which is built to run highly parallel jobs. Based on all these infrastructure difference, our hypothesis is that AWS-EC2 will provide slower performance compared to INDY2 and ARCHER.


In running the simulation, I had help from Dr. Rupert Nash to run HemeLB simulation on ARCHER and INDY2. I need help because I did not have access to both of these infrastructures. On AWS-EC2 infrastructure, however, I run the HemeLB scenario myself. HemeLB simulation was run using the HemeWeb interface on AWS.




